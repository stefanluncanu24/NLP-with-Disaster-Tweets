{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Our Deeds are the Reason of this #earthquake M...\n",
      "1                Forest fire near La Ronge Sask. Canada\n",
      "2     All residents asked to 'shelter in place' are ...\n",
      "3     13,000 people receive #wildfires evacuation or...\n",
      "4     Just got sent this photo from Ruby #Alaska as ...\n",
      "5     #RockyFire Update => California Hwy. 20 closed...\n",
      "6     #flood #disaster Heavy rain causes flash flood...\n",
      "7     I'm on top of the hill and I can see a fire in...\n",
      "8     There's an emergency evacuation happening now ...\n",
      "9     I'm afraid that the tornado is coming to our a...\n",
      "10          Three people died from the heat wave so far\n",
      "11    Haha South Tampa is getting flooded hah- WAIT ...\n",
      "12    #raining #flooding #Florida #TampaBay #Tampa 1...\n",
      "13              #Flood in Bago Myanmar #We arrived Bago\n",
      "14    Damage to school bus on 80 in multi car crash ...\n",
      "15                                       What's up man?\n",
      "16                                        I love fruits\n",
      "17                                     Summer is lovely\n",
      "18                                    My car is so fast\n",
      "19                         What a goooooooaaaaaal!!!!!!\n",
      "20                               this is ridiculous....\n",
      "21                                    London is cool ;)\n",
      "22                                          Love skiing\n",
      "23                                What a wonderful day!\n",
      "24                                             LOOOOOOL\n",
      "25                       No way...I can't eat that shit\n",
      "26                                Was in NYC last week!\n",
      "27                                   Love my girlfriend\n",
      "28                                            Cooool :)\n",
      "29                                   Do you like pasta?\n",
      "30                                             The end!\n",
      "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...\n",
      "32    We always try to bring the heavy. #metal #RT h...\n",
      "33    #AFRICANBAZE: Breaking news:Nigeria flag set a...\n",
      "34                   Crying out for more! Set me ablaze\n",
      "35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...\n",
      "36    @PhDSquares #mufc they've built so much hype a...\n",
      "37    INEC Office in Abia Set Ablaze - http://t.co/3...\n",
      "38    Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...\n",
      "39                               Ablaze for you Lord :D\n",
      "40    Check these out: http://t.co/rOI2NSmEJJ http:/...\n",
      "41    on the outside you're ablaze and alive\\nbut yo...\n",
      "42    Had an awesome time visiting the CFC head offi...\n",
      "43         SOOOO PUMPED FOR ABLAZE ???? @southridgelife\n",
      "44    I wanted to set Chicago ablaze with my preachi...\n",
      "45    I gained 3 followers in the last week. You? Kn...\n",
      "46    How the West was burned: Thousands of wildfire...\n",
      "47    Building the perfect tracklist to life leave t...\n",
      "48    Check these out: http://t.co/rOI2NSmEJJ http:/...\n",
      "49    First night with retainers in. It's quite weir...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train[\"text\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text'], dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>328</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ready to get annihilated for the BUCS game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>443</td>\n",
       "      <td>apocalypse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2619</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>3640</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Quilmes , Arg</td>\n",
       "      <td>This desperation dislocation\\nSeparation conde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>3900</td>\n",
       "      <td>devastated</td>\n",
       "      <td>PG Chillin!</td>\n",
       "      <td>Man Currensy really be talkin that talk... I'd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>4342</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Going to a fest? Bring swimming goggles for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>5781</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campsite recommendations \\nToilets /shower \\nP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>6552</td>\n",
       "      <td>injury</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>My prediction for the Vikings game this Sunday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>6554</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dante Exum's knee injury could stem Jazz's hop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>6570</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sport_EN Just being linked to Arsenal causes ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>6701</td>\n",
       "      <td>lava</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Imagine a room with walls that are lava lamps.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>6729</td>\n",
       "      <td>lava</td>\n",
       "      <td>Clayton, NC</td>\n",
       "      <td>Check out my Lava lamp dude ???? http://t.co/T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>6861</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>i'm a Citizen of the World</td>\n",
       "      <td>If abortion is murder then blowjobs are cannib...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>7226</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>on to the next adventure</td>\n",
       "      <td>Of course the one day I have to dress professi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0             keyword                    location  \\\n",
       "229          328         annihilated                         NaN   \n",
       "301          443          apocalypse                         NaN   \n",
       "356          513                army                      Studio   \n",
       "1822        2619             crashed                         NaN   \n",
       "2536        3640          desolation               Quilmes , Arg   \n",
       "2715        3900          devastated                 PG Chillin!   \n",
       "3024        4342        dust%20storm                     chicago   \n",
       "4068        5781      forest%20fires                         NaN   \n",
       "4609        6552              injury                  Saint Paul   \n",
       "4611        6554              injury                         NaN   \n",
       "4622        6570              injury                         NaN   \n",
       "4713        6701                lava               Nashville, TN   \n",
       "4714        6702                lava  probably watching survivor   \n",
       "4732        6729                lava                 Clayton, NC   \n",
       "4820        6861       mass%20murder  i'm a Citizen of the World   \n",
       "5068        7226  natural%20disaster    on to the next adventure   \n",
       "\n",
       "                                                   text  target  \n",
       "229          Ready to get annihilated for the BUCS game       1  \n",
       "301   Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the...       1  \n",
       "356   But if you build an army of 100 dogs and their...       1  \n",
       "1822  My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVH...       1  \n",
       "2536  This desperation dislocation\\nSeparation conde...       1  \n",
       "2715  Man Currensy really be talkin that talk... I'd...       1  \n",
       "3024  Going to a fest? Bring swimming goggles for th...       1  \n",
       "4068  Campsite recommendations \\nToilets /shower \\nP...       1  \n",
       "4609  My prediction for the Vikings game this Sunday...       1  \n",
       "4611  Dante Exum's knee injury could stem Jazz's hop...       1  \n",
       "4622  @Sport_EN Just being linked to Arsenal causes ...       1  \n",
       "4713     Imagine a room with walls that are lava lamps.       1  \n",
       "4714  The sunset looked like an erupting volcano ......       1  \n",
       "4732  Check out my Lava lamp dude ???? http://t.co/T...       1  \n",
       "4820  If abortion is murder then blowjobs are cannib...       1  \n",
       "5068  Of course the one day I have to dress professi...       1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/wrrosa/keras-bert-using-tfhub-modified-train-data#About-this-kernel\n",
    "\n",
    "# There are targets which are wrong\n",
    "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "train[train['Unnamed: 0'].isin(ids_with_target_error)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the target values\n",
    "train.loc[train['Unnamed: 0'].isin(ids_with_target_error),'target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set the NLTK_DATA environment variable to your dir\n",
    "nltk_data_dir = '/home/stefan/nltk_data' \n",
    "os.environ['NLTK_DATA'] = nltk_data_dir\n",
    "\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    os.makedirs(nltk_data_dir)\n",
    "\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://www.kaggle.com/code/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert#5.-Data-Cleaning-\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    for word in text.split():\n",
    "        if word.lower() in abbreviations:\n",
    "            text = text.replace(word, abbreviations[word.lower()])\n",
    "\n",
    "    text = re.sub(r'(.)\\1{1,}', r'\\1', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_preprocess = ['text', 'keyword', 'location']\n",
    "\n",
    "for column in columns_to_preprocess:\n",
    "    train[column] = train[column].fillna('Missing')\n",
    "    test[column] = test[column].fillna('Missing')\n",
    "    \n",
    "train['clean_text'] = train['text'].apply(preprocess)\n",
    "test['clean_text'] = test['text'].apply(preprocess)\n",
    "\n",
    "train['clean_keyword'] = train['keyword'].apply(preprocess)\n",
    "test['clean_keyword'] = test['keyword'].apply(preprocess)\n",
    "\n",
    "train['clean_location'] = train['location'].apply(preprocess)\n",
    "test['clean_location'] = test['location'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_keyword</th>\n",
       "      <th>clean_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>hapened terible car crash</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake diferent cities stay safe eve...</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond gese fleing acros stret ...</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhon soudelor kils china taiwan</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   0  Missing  Missing                 Just happened a terrible car crash   \n",
       "1   2  Missing  Missing  Heard about #earthquake is different cities, s...   \n",
       "2   3  Missing  Missing  there is a forest fire at spot pond, geese are...   \n",
       "3   9  Missing  Missing           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11  Missing  Missing      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                          clean_text clean_keyword  \\\n",
       "0                          hapened terible car crash        mising   \n",
       "1  heard earthquake diferent cities stay safe eve...        mising   \n",
       "2  forest fire spot pond gese fleing acros stret ...        mising   \n",
       "3              apocalypse lighting spokane wildfires        mising   \n",
       "4                  typhon soudelor kils china taiwan        mising   \n",
       "\n",
       "  clean_location  \n",
       "0         mising  \n",
       "1         mising  \n",
       "2         mising  \n",
       "3         mising  \n",
       "4         mising  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_keyword</th>\n",
       "      <th>clean_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deds reason earthquake may alah forgive us</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified oficers...</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>mising</td>\n",
       "      <td>mising</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  keyword location  \\\n",
       "0           1  Missing  Missing   \n",
       "1           4  Missing  Missing   \n",
       "2           5  Missing  Missing   \n",
       "3           6  Missing  Missing   \n",
       "4           7  Missing  Missing   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text clean_keyword  \\\n",
       "0         deds reason earthquake may alah forgive us        mising   \n",
       "1              forest fire near la ronge sask canada        mising   \n",
       "2  residents asked shelter place notified oficers...        mising   \n",
       "3  people receive wildfires evacuation orders cal...        mising   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...        mising   \n",
       "\n",
       "  clean_location  \n",
       "0         mising  \n",
       "1         mising  \n",
       "2         mising  \n",
       "3         mising  \n",
       "4         mising  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'keyword', 'location', 'text', 'target', 'clean_text',\n",
       "       'clean_keyword', 'clean_location'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'clean_text', 'clean_keyword',\n",
       "       'clean_location'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['text', 'keyword', 'location'], inplace=True)\n",
    "test.drop(columns=['text', 'keyword', 'location'], inplace=True)\n",
    "\n",
    "train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['clean_keyword', 'clean_location'], inplace=True)\n",
    "test.drop(columns=['clean_keyword', 'clean_location'], inplace=True)\n",
    "\n",
    "train.to_csv('train_cleaned.csv', index=False)\n",
    "test.to_csv('test_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stefan\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Stefan\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247823db49714f61bd517b8babe0d4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6848, 'grad_norm': 4.139852523803711, 'learning_rate': 2.5e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5844, 'grad_norm': 4.621327877044678, 'learning_rate': 5e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5005, 'grad_norm': 3.2949280738830566, 'learning_rate': 2.252747252747253e-05, 'epoch': 0.79}\n",
      "{'train_runtime': 762.26, 'train_samples_per_second': 7.989, 'train_steps_per_second': 0.251, 'train_loss': 0.5655983630275228, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9e1fb36a99464f913a207a7ecb3a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43749499320983887, 'eval_accuracy': 0.8115561391989494, 'eval_f1': 0.8084657174758022, 'eval_precision': 0.8138905638033413, 'eval_recall': 0.8115561391989494, 'eval_runtime': 50.6106, 'eval_samples_per_second': 30.093, 'eval_steps_per_second': 0.474, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b88d7bf8824a73b0b5d2bff8cac1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train['clean_text'].tolist(),\n",
    "    train['target'].tolist(),\n",
    "    test_size=0.2,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test['clean_text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None): \n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "test_dataset = CustomDataset(test_encodings) \n",
    "\n",
    "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=1,              \n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=100,               \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=50,                \n",
    "    evaluation_strategy='steps',     \n",
    "    eval_steps=1000,                \n",
    "    save_steps=2000,                 \n",
    "    load_best_model_at_end=True,     \n",
    "    metric_for_best_model='f1',     \n",
    "    greater_is_better=True,         \n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,           \n",
    "    compute_metrics=compute_metrics      \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "val_results = trainer.evaluate()\n",
    "print(val_results)\n",
    "\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "preds = test_predictions.predictions.argmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['target'] = preds\n",
    "submission.to_csv('submission_deberta.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
